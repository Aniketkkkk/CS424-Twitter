{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NATURAL LANGUAGE PROCESSING\n",
    "\n",
    "## Assignment 3 - Vectorization\n",
    "\n",
    "Group members : \n",
    "\n",
    "1. Bharath Vedartham (01FB16ECS439)\n",
    "2. Aniket Kumar (01FB16ECS057)\n",
    "3. Dinesh Sathyanarayanan (01FB16ECS114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing functions\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english') and word!='ie':\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_stopwords(words.split())\n",
    "    words = lemmatize_words(words)\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into pd dataframe\n",
    "df = pd.read_csv('dataset.csv', usecols=['text', 'stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing to serve purpose of binary classifcation\n",
    "df[df.stars!=3]\n",
    "df.loc[df.stars == 1, 'stars'] = 0\n",
    "df.loc[df.stars == 5, 'stars'] = 1\n",
    "df.loc[df.stars == 4, 'stars'] = 1\n",
    "df.loc[df.stars == 2, 'stars'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Equal number of samples for each class\n",
    "first_part = df[df.stars == 0]\n",
    "second_part = df[df.stars == 1]\n",
    "second_part = second_part[0:len(first_part)]\n",
    "\n",
    "# Concat to single frame\n",
    "frames = [first_part, second_part]\n",
    "result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1 : Baseline case - Linear classification using extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy import lexicon_methods\n",
    "import textacy\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import progressbar\n",
    "textacy.lexicon_methods.download_depechemood(data_dir=None, force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Feature Extraction***\n"
     ]
    }
   ],
   "source": [
    "# Preprocess using textacy and get 8 emotion valence\n",
    "# features[0] - valence\n",
    "# features[1] - sentiment polarity\n",
    "\n",
    "features = np.zeros((len(first_part)*2, 9))\n",
    "label = np.zeros(len(first_part)*2)\n",
    "allreviews = []\n",
    "print(\"***Feature Extraction***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n"
     ]
    }
   ],
   "source": [
    "bar = progressbar.ProgressBar(maxval=len(first_part)*2, \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "\n",
    "bar.start()\n",
    "for i, row in enumerate(result.iterrows()):\n",
    "    bar.update(i+1)\n",
    "    text=textacy.preprocess.remove_punct(row[1]['text'])\n",
    "    text = textacy.preprocess.replace_numbers(text, replace_with='*NUMBER*')\n",
    "    text = textacy.preprocess.replace_emails(text, replace_with='*EMAIL*')\n",
    "    text = textacy.preprocess.replace_phone_numbers(text, replace_with='*PHONE*')\n",
    "    text = textacy.preprocess.replace_urls(text, replace_with='*URL*')\n",
    "    text=textacy.preprocess.replace_currency_symbols(text,replace_with='INR')\n",
    "    text = textacy.preprocess_text(text,lowercase=True)\n",
    "    allreviews.append(normalize(text))\n",
    "    doc = textacy.Doc(text, lang='en_core_web_sm')\n",
    "    ddict=lexicon_methods.emotional_valence(doc, threshold=0.0, dm_data_dir=None, dm_weighting='normfreq')    \n",
    "    features[i][0] = ddict['AFRAID'];features[i][1] = ddict['AMUSED']\n",
    "    features[i][2] = ddict['ANGRY'];features[i][3] = ddict['ANNOYED']\n",
    "    features[i][4] = ddict['DONT_CARE'];features[i][5] = ddict['HAPPY']\n",
    "    features[i][6] = ddict['INSPIRED'];features[i][7] = ddict['SAD']\n",
    "    features[i][8] = TextBlob(text).sentiment.polarity\n",
    "    label[i] = row[1]['stars']\n",
    "bar.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3352, 9)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3352,)\n"
     ]
    }
   ],
   "source": [
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data - 67%   Test data - 33%\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2245, 9) (2245,) (1107, 9) (1107,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM linear classification using sklearn \n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto', kernel='linear')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7949412827461608\n"
     ]
    }
   ],
   "source": [
    "# Classification accuracy on test data\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2 : Linear classification using sparse vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15802\n",
      "[8.42446357 7.73131639 7.1717006  ... 8.42446357 8.42446357 8.42446357]\n"
     ]
    }
   ],
   "source": [
    "# allreviews - list of text documents\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(allreviews)\n",
    "\n",
    "# Print vocabulary\n",
    "print(len(vectorizer.vocabulary_))\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode each review\n",
    "vectors = []\n",
    "for i in allreviews:\n",
    "    vectors.append(vectorizer.transform([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3352\n"
     ]
    }
   ],
   "source": [
    "print(len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3352\n"
     ]
    }
   ],
   "source": [
    "vect = []\n",
    "for i in vectors:\n",
    "    vect.append(i.toarray()[0])\n",
    "print(len(vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3352, 15802)\n"
     ]
    }
   ],
   "source": [
    "vect = np.array(vect)\n",
    "print(vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data - 67%   Test data - 33%\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(vect, label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2245, 15802) (2245,) (1107, 15802) (1107,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM linear classification using sklearn \n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto', kernel='linear')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8699186991869918\n"
     ]
    }
   ],
   "source": [
    "# Classification accuracy on test data\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3 : Dense vector strategy with SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = svd.fit_transform(X_train)\n",
    "# result is of shape (2245, 100)\n",
    "# so dimensionality is reduced to 100 from 15802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2245, 100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM linear classification using sklearn \n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto', kernel='linear')\n",
    "clf.fit(result, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6187895212285456\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(svd.fit_transform(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with more dimensions\n",
    "svd1 = TruncatedSVD(n_components=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2245, 15802)\n",
      "(2245, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "result = svd1.fit_transform(X_train)\n",
    "# result is of shape (2245, 1000)\n",
    "# so dimensionality is reduced to 1000 from 15802\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM linear classification using sklearn \n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto', kernel='linear')\n",
    "clf.fit(result, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1107, 15802)\n",
      "(2245, 15802)\n",
      "(1107, 1000)\n",
      "0.6196928635953026\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(X_train.shape)\n",
    "print(svd1.fit_transform(X_test).shape)\n",
    "print(clf.score(svd1.fit_transform(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4 : Non-linear classification using Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import progressbar\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dines\\anaconda3\\envs\\python35_t\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors['hello'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dines\\anaconda3\\envs\\python35_t\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if sys.path[0] == '':\n",
      "[========================================================================] 100%\n"
     ]
    }
   ],
   "source": [
    "bar = progressbar.ProgressBar(maxval=len(allreviews), \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "bar.start()\n",
    "emb = np.zeros([len(allreviews),300])\n",
    "for i1,i in enumerate(allreviews):\n",
    "    bar.update(i1+1)\n",
    "    words = i.split()\n",
    "    for j in words:\n",
    "        if(j not in model.vocab):\n",
    "            pass\n",
    "        else:\n",
    "            emb[i1] = np.sum([model.wv[j], emb[i1]], axis=0)\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3352, 300)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2245, 300)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(emb, label, test_size=0.33, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All keras modules for ANN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential model for ANN classification (non-linear)\n",
    "\n",
    "# Input dimensions - 300 (from google vectors)\n",
    "model = Sequential()\n",
    "\n",
    "# First layer (input layer)\n",
    "model.add(Dense(64, input_dim=300, activation='relu'))\n",
    "\n",
    "# Dropout after each layer for reducing overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Second layer (hidden)\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer (1 neuron for binary classification)\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                19264     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 23,489\n",
      "Trainable params: 23,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2245/2245 [==============================] - ETA: 1:30 - loss: 3.3572 - acc: 0.484 - ETA: 1s - loss: 1.6211 - acc: 0.5379  - 6s 2ms/step - loss: 1.4453 - acc: 0.5425\n",
      "Epoch 2/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.9987 - acc: 0.507 - ETA: 0s - loss: 0.7993 - acc: 0.564 - 0s 35us/step - loss: 0.7781 - acc: 0.5777\n",
      "Epoch 3/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.6441 - acc: 0.601 - ETA: 0s - loss: 0.6686 - acc: 0.597 - 0s 36us/step - loss: 0.6678 - acc: 0.6058\n",
      "Epoch 4/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.7704 - acc: 0.578 - ETA: 0s - loss: 0.6390 - acc: 0.655 - 0s 33us/step - loss: 0.6382 - acc: 0.6530\n",
      "Epoch 5/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.5576 - acc: 0.718 - ETA: 0s - loss: 0.5745 - acc: 0.700 - 0s 33us/step - loss: 0.5747 - acc: 0.6971\n",
      "Epoch 6/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.7326 - acc: 0.656 - ETA: 0s - loss: 0.5931 - acc: 0.717 - 0s 37us/step - loss: 0.5860 - acc: 0.7145\n",
      "Epoch 7/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.5048 - acc: 0.742 - ETA: 0s - loss: 0.5422 - acc: 0.754 - 0s 32us/step - loss: 0.5382 - acc: 0.7599\n",
      "Epoch 8/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.5862 - acc: 0.718 - ETA: 0s - loss: 0.5111 - acc: 0.759 - 0s 33us/step - loss: 0.5069 - acc: 0.7661\n",
      "Epoch 9/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.4797 - acc: 0.742 - ETA: 0s - loss: 0.5055 - acc: 0.758 - 0s 41us/step - loss: 0.4974 - acc: 0.7653\n",
      "Epoch 10/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.5435 - acc: 0.742 - ETA: 0s - loss: 0.4985 - acc: 0.787 - 0s 40us/step - loss: 0.4878 - acc: 0.7826\n",
      "Epoch 11/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.4461 - acc: 0.804 - ETA: 0s - loss: 0.4509 - acc: 0.800 - 0s 41us/step - loss: 0.4594 - acc: 0.7938\n",
      "Epoch 12/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.4496 - acc: 0.812 - ETA: 0s - loss: 0.4494 - acc: 0.802 - 0s 33us/step - loss: 0.4521 - acc: 0.8027\n",
      "Epoch 13/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.3605 - acc: 0.875 - ETA: 0s - loss: 0.4405 - acc: 0.815 - 0s 34us/step - loss: 0.4444 - acc: 0.8147\n",
      "Epoch 14/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.3696 - acc: 0.843 - ETA: 0s - loss: 0.4407 - acc: 0.825 - 0s 37us/step - loss: 0.4398 - acc: 0.8209\n",
      "Epoch 15/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.4239 - acc: 0.773 - ETA: 0s - loss: 0.4086 - acc: 0.831 - 0s 38us/step - loss: 0.4084 - acc: 0.8312\n",
      "Epoch 16/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.3607 - acc: 0.859 - ETA: 0s - loss: 0.3819 - acc: 0.837 - 0s 38us/step - loss: 0.3800 - acc: 0.8379\n",
      "Epoch 17/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.3661 - acc: 0.820 - ETA: 0s - loss: 0.4193 - acc: 0.827 - 0s 36us/step - loss: 0.3990 - acc: 0.8334\n",
      "Epoch 18/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.4559 - acc: 0.867 - ETA: 0s - loss: 0.3836 - acc: 0.830 - 0s 32us/step - loss: 0.3871 - acc: 0.8298\n",
      "Epoch 19/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.4906 - acc: 0.789 - ETA: 0s - loss: 0.3822 - acc: 0.846 - 0s 33us/step - loss: 0.3860 - acc: 0.8419\n",
      "Epoch 20/20\n",
      "2245/2245 [==============================] - ETA: 0s - loss: 0.4068 - acc: 0.828 - ETA: 0s - loss: 0.3651 - acc: 0.842 - 0s 34us/step - loss: 0.3653 - acc: 0.8410\n",
      "1107/1107 [==============================] - ETA:  - 0s 57us/step\n",
      "\n",
      "Accuracy is 84.19150863559588\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train, epochs = 20, batch_size = 128)\n",
    "loss_and_metrics = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print(\"\\nAccuracy is {}\".format(loss_and_metrics[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4 : Non-linear classification using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2245, 300)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1107, 300)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(2245,20,15,1)\n",
    "X_test = X_test.reshape(1107,20,15,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "#create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(20,15,1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 18, 13, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 11, 32)        18464     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 5632)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 5633      \n",
      "=================================================================\n",
      "Total params: 24,737\n",
      "Trainable params: 24,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
